{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# üöÄ **Exploring Perplexity API Functionality**\n",
    "\n",
    "This notebook aims to explore Perplexity API functionalities. [pplx-api](https://docs.perplexity.ai/). \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GiacomoMeloni/ExploringLLMs/blob/main/notebooks/exploring_perplexityai_api.ipynb)\n",
    "\n",
    "## ü§î **About Perplexity AI**\n",
    "\n",
    "[Perplexity AI](https://www.perplexity.ai/) is an AI-powered search engine and chatbot that uses advanced natural language processing tequniques to provide users with accurate and up-to-date information on various topics. It is designed to search the web in real-time and offer information on a wide range of subjects. Some key features and capabilities of Perplexity AI include:\n",
    "\n",
    "- 1Ô∏è‚É£ **Answer Engine**: Perplexity AI focuses on advancing how people [discover](https://www.perplexity.ai/discover) and share information by providing ready-made answers to users' questions and citing sources in real-time.\n",
    "\n",
    "- 2Ô∏è‚É£ **Multifaceted Applications**: Perplexity AI is versatile and can assist various professions, such as researchers, writers, artists, musicians, and programmers, in tasks like answering questions, generating text, writing creative content, and summarizing text.\n",
    "\n",
    "- 3Ô∏è‚É£ **Accuracy**: The AI uses natural language processing and machine learning techniques to analyze text data, providing an in-depth analysis and generating fresh concepts, resulting in accurate answers and information.\n",
    "\n",
    "- 4Ô∏è‚É£ **User-friendly Interface**: Perplexity AI is available on the web and as an app for iPhone users, making it easily accessible to a wide range of users.\n",
    "\n",
    "Perplexity AI was founded in 2022 by Andy Konwinski, Aravind Srinivas, Denis Yarats, and Johnny Ho, who have experience working with large language models at Google AI.\n",
    "\n",
    "\n",
    "## üì∞ **Recent releases**\n",
    "\n",
    "Perplexity AI recently introduced its [pplx-api](https://docs.perplexity.ai/), which allows developers to access the company's AI models and other resources through a REST API. The API is designed to work with various large language models:\n",
    "\n",
    "| Model Name              | Context Length | Model Type        |\n",
    "|-------------------------|-----------------|-------------------|\n",
    "| codellama-34b-instruct  | 16384           | Chat Completion  |\n",
    "| llama-2-70b-chat        | 4096            | Chat Completion  |\n",
    "| mistral-7b-instruct     | 4096 ‚¨Ü     | Chat Completion  |\n",
    "| pplx-7b-chat            | 8192            | Chat Completion  |\n",
    "| pplx-70b-chat           | 4096            | Chat Completion  |\n",
    "| pplx-7b-online          | 4096            | Chat Completion  |\n",
    "| pplx-70b-online         | 4096            | Chat Completion  |\n",
    "\n",
    "‚¨ÜÔ∏è as reported on the üó∫Ô∏è [roadmap](https://docs.perplexity.ai/docs/feature-roadmap) the context length of Mistral 7b will be extended to 32K tokens.*"
   ],
   "metadata": {
    "id": "8rx6ZjAdA6ID"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EqiRcQg_ZgS",
    "outputId": "a9848898-fcb2-4c31-ddcd-902693d24cf4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from time import time\n",
    "from enum import Enum\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚ÑπÔ∏è Get started\n",
    "\n",
    "As reported in the [official documentation](https://docs.perplexity.ai/docs) is it possible to access to pplx-api generating an API Key through an active account. Pricing details are reported in the [pricing page](https://docs.perplexity.ai/docs/pricing).\n",
    "\n",
    "Is it possible to follow the official [API Reference](https://docs.perplexity.ai/reference/post_chat_completions). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Useful placeholders\n",
    "\n",
    "# PerplexityAI's API endpoint to access generative models\n",
    "URL = \"https://api.perplexity.ai/chat/completions\"\n",
    "\n",
    "# Is it possibile to specify some parameters for the output generation of each model\n",
    "MAX_TOKENS = 1500\n",
    "TEMPERATURE = 0\n",
    "TOP_P = 1\n",
    "FREQUENCY_PENALTY = 1.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ü§ñ Model Details \n",
    "Currently, PerplexityAI APIs offer access to open-source models as listed in [Supported Models] page. However, as reported in their blog, the models with native web access managed by PerplexityAI APIs are `pplx-7b-online` and `pplx-70b-online`.\n",
    "\n",
    "These models have been fine-tuned to effectively use snippets to inform their responses and are regularly updated to continually improve perfomance. Respectively, `pplx-7b-online` was finetuned using `mistral-7b`, and `pplx-70b-online` was fine-tuned using `llama2-70b`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Available models from PerplexityAI API\n",
    "\n",
    "class AvailablePPLXModels(Enum):\n",
    "  MISTRAL_7B: str = \"mistral-7b-instruct\"\n",
    "  PPLX_7B: str = \"pplx-7b-chat\"\n",
    "  PPLX_70B: str = \"pplx-70b-chat\"\n",
    "  PPLX_7B_ON: str = \"pplx-7b-online\" #üõú Native Online Retrieval\n",
    "  PPLX_70B_ON: str = \"pplx-70b-online\" #üõú Native Online Retrieval\n",
    "  LLAMA2_70B: str = \"llama-2-70b-chat\"\n",
    "  CODE_LLAMA_34B: str = \"codellama-34b-isntruct\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-QJiX0GoHpp",
    "outputId": "73a4dd73-d4f8-4452-8e97-94ae3f221a44"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîé Set up Authorization Header\n",
    "\n",
    "As you can find in the **Get Started** section, it is necessary to use an API KEY to interact with PerplexityAI APIs. \n",
    "\n",
    "* If you are using a Jupyter Notebook insert `PPLXAI_API_TOKEN` as environment variable to retrieve the key;\n",
    "* If you are using Google Colab insert `PPLXAI_API_TOKEN` as secret using the newly launched Secrets tab to retrieve the key."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        pplx_api_key = userdata.get('PPLXAI_API_TOKEN')\n",
    "    except Exception: \n",
    "        pplx_api_key = None\n",
    "else:\n",
    "    pplx_api_key = os.environ.get('PPLXAI_API_TOKEN', None)\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": f\"Bearer {pplx_api_key}\"\n",
    "}\n",
    "\n",
    "assert headers['authorization'] is not None, \"You need to add an API Key to use Perplexity's API\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üéôÔ∏è Preparing Payload\n",
    "\n",
    "To interact with the model we can use the classic message list as openAI's chat completion APIs. For more details is possible to check on [Perplexity API Reference](https://docs.perplexity.ai/reference/post_chat_completions). Furthermore, is it possible to specify parameters values to tune LLM's output behaviour. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('Insert your query here')\n",
    "user_query = input('User: ')\n",
    "print(f'Given query:\\t{user_query}')\n",
    "\n",
    "sys_prompt = f\"\"\"You have to report the most recent information requested by the user. Use only official documentation and be as concise as possible.\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": AvailablePPLXModels.PPLX_7B_ON.value,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"top_p\": TOP_P,\n",
    "    \"frequency_penalty\": FREQUENCY_PENALTY\n",
    "}\n",
    "\n",
    "assert payload['messages'] is not None, \"You need to specify some message for the model to work!\"\n",
    "\n",
    "print(f'Payload ready!')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyxbtrqVCTxy",
    "outputId": "da3d1eaa-57b0-4947-ca25-84761305743f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start = time()\n",
    "response = requests.post(URL, json=payload, headers=headers).json()\n",
    "print(f\"Execution time: {round(time()-start, 2)}s\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI_p4OrdFZXi",
    "outputId": "55f1beb7-7a2e-4a03-f33e-8903eb7e99ff"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üì¢ Response details\n",
    "It is possible to retrieve the total amount of tokens the model dealt with for the current interation. Unfortunately, it is not possible to track the sources used by the LLM to generate the output. This makes difficult to evaluate the model retrieval system and understand how well the model is working.    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Input tokens:\\t {response['usage']['prompt_tokens']}\")\n",
    "print(f\"Output tokens:\\t {response['usage']['completion_tokens']}\")\n",
    "print(f\"Total tokens:\\t {response['usage']['total_tokens']}\\n\\n\")\n",
    "\n",
    "print(f\"{response['choices'][0]['message']['content']}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSh1wZNSqGs-",
    "outputId": "c64a87b2-8177-487e-b3f3-f638079e2aba"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "‚ö†Ô∏è Some information reported here have been retrieved leveraging `pplx-7b-online`!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
