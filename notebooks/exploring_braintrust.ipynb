{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# üöÄ **Exploring Braintrust Evaluation APIs**\n",
    "\n",
    "This notebook aims to explore Evaluation APIs offered by [Braintrust](https://docs.perplexity.ai/). \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GiacomoMeloni/ExploringLLMs/blob/main/notebooks/exploring_braintrust.ipynb)\n",
    "\n",
    "## ü§î  **About Braintrust**\n",
    "Braintrust is a service that helps you evaluate your LLM app, mainly focused on production-quality applications. It is possible to leverage on the [Python SDK](https://www.braintrustdata.com/docs/libs/python) as well as on the [Node library](https://www.braintrustdata.com/docs/libs/nodejs).\n",
    "\n",
    "Evaluations are crucial in AI, and accuracy deserves as much focus as speed in developing LLM-based applications. Braintrust's evaluation-driven approach enhances product reliability and quality, accelerating the testing process. This service allows teams to share evaluation datasets, with also versioning features; define their own evaluation metrics or leveraging in predefined metrics offered by the framework.\n",
    "\n",
    "## üîß **Get Started**\n",
    "To access to `Braintrust` Dashboard an API key is needed. You can easily access through the \"Settings\" page of active account (is needed to sign up on the platform) and click on \"API Keys\" to generate a new key for your projects.\n",
    "\n",
    "‚ö†Ô∏è **Remember API Keys will be visibile only one time, after that they won't be displayed again! So keep it safe once created!**\n",
    "‚ÑπÔ∏è **The API key needs to be stored as environment variable with the following name `BRAINTRUST_API_KEY`**\n",
    "\n",
    "Another feature that needs mention is that you need to update a GenAI Provider API Key (e.g. OpenAI, Anthropic, etc.) to leverage on some model-based metrics that we will cover later on this notebook.\n",
    "\n",
    "‚ÑπÔ∏è **There is no need to have an enterprise plan to leverage on basic evaluation features offered by the library**\n",
    "‚ÑπÔ∏è **For more details on API key sey-up refer to [Create an API key](https://www.braintrustdata.com/docs#create-an-api-key) guide section**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28a87194f7184aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install braintrust autoevals openai pandas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b6129062683654"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Basic check to get the environment in which the notebook is running\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    COLAB_ENV = True\n",
    "    from google.colab import userdata\n",
    "else:\n",
    "    COLAB_ENV = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac34d674e434181"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üßëüèº‚ÄçüöÄ Set up\n",
    "To access to all the functionalities offered by Braintrust we need to use a Braintrust API Key. Additionally, in this notebook is illustrated a basic test use case that leverages on OpenAI gpt-3.5-turbo model so an OpenAI API key is needed as well.\n",
    "\n",
    "üóùÔ∏èTo make this notebook work is needed to set up the following environment variables:\n",
    "\n",
    "\n",
    "| Env Variable              | Description                                          |\n",
    "|---------------------------|------------------------------------------------------|\n",
    "| `BRAINTRUST_API_KEY`      | Braintrust API key                                   |\n",
    "| `OPENAI_API_KEY`          | OpenAI API key (Optional)                            |\n",
    "| `PPLXAI_API_KEY`          | PerplexityAI API key                                 |\n",
    "| `TRUSTBRAIN_PROJECT_NAME` | The name assigned to Braintrust's Evaluation Project | "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "487f13872e893c90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from braintrust import Eval, init_dataset\n",
    "from datetime import datetime\n",
    "from autoevals import Factuality, LevenshteinScorer \n",
    "\n",
    "check_bt_api_key = bool(os.environ.get('BRAINTRUST_API_KEY')) if not COLAB_ENV else userdata.get('BRAINTRUST_API_KEY')\n",
    "\n",
    "PROJECT_NAME: str = os.environ.get('TRUSTBRAIN_PROJECT_NAME')\n",
    "\n",
    "assert check_bt_api_key, \"‚ùå A Braintrust API Key it's needed for the purpose of this notebook!\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba1f9f17fab07705"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üõú AI Proxy\n",
    "Braintrust provides also an AI proxy to access to different LLM-providers and models using a single API. For the sake of this guide is used a PerplexityAI API key. It is possible to use OpenAI, Anthropic or other providers API keys as well. \n",
    "\n",
    "‚ÑπÔ∏è More information are provided in the official [AI Proxy](https://www.braintrustdata.com/docs/guides/proxy) guide section."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14f87f1c71efc52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For this project is used a PerplexityAI API key. It is possible to use OpenAI, Anthropic or other providers API keys as well.\n",
    "client_key = os.environ.get('PPLXAI_API_KEY', None) if not COLAB_ENV else userdata.get('PPLXAI_API_KEY')\n",
    "\n",
    "# client_key = os.environ.get('OPENAI_API_KEY', None) if not COLAB_ENV else userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://braintrustproxy.com/v1\",\n",
    "  api_key=client_key\n",
    ")\n",
    "\n",
    "assert client.api_key is not None, \"‚ùå An LLM-provider API Key is needed for the purpose of this notebook!\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eddd92adeed9771"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí° Use Case: Sentiment Classifier\n",
    "This demo use case involves a custom classifier to detect positive or negative products reviews.\n",
    "\n",
    "As first step, an evaluation dataset needs to be defined. This dataset consist in list of objects composed by the following elements: \n",
    "\n",
    "* `input`: the input to evaluate, in this scenario it will be the review but for example if you need to evaluate a question answering model it might be a question;\n",
    "* `output`: the output value is supposed to be the expected application response (you can rename this as *expected* directly from your evaluation dataset to align directly to what the framework is expecting to receive);\n",
    "* `metadata`: is a set of key-value pairs that you can use to filter and group your data.\n",
    "\n",
    "This dataset will be used to evaluate the LLM-based application!\n",
    "\n",
    "Is it possible to provide this data by loading a CSV file. Use any file type you prefer as long as it allows you to create a list of dictionaries as done in the cell below. *You can find a sample dataset in *üìÇ*resources/braintrust/ folder*   \n",
    "\n",
    "‚ÑπÔ∏è For further information about Evaluation Datasets look at [Datasets](https://www.braintrustdata.com/docs/guides/datasets) page in the official guide."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea473de609260676"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Dataset...\n",
      "Test record found: 5\n",
      "Evaluation Dataset loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               input  expected\n0  The product exceeded my expectations. Even Gan...  POSITIVE\n1                                 How's the weather?        NA\n2  Terrible experience with this product. I would...  NEGATIVE\n3  The service was not professional, how could yo...  NEGATIVE\n4  Great customer service, very helpful and respo...  POSITIVE",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>expected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The product exceeded my expectations. Even Gan...</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How's the weather?</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Terrible experience with this product. I would...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The service was not professional, how could yo...</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Great customer service, very helpful and respo...</td>\n      <td>POSITIVE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RELATIVE_PATH = os.path.join('resources', 'braintrust', 'eval_data_sentiment_classifier.csv')\n",
    "assert os.path.exists(RELATIVE_PATH), \"‚ùå Evaluation Dataset not found!\"\n",
    "print(f\"Loading Evaluation Dataset...\")\n",
    "eval_df = pd.read_csv(RELATIVE_PATH, sep=';', encoding='utf-8', index_col=None, keep_default_na=False)\n",
    "\n",
    "eval_dataset = [{\"input\": test_review,\"expected\": expected_output} \n",
    "                for test_review, expected_output in zip(eval_df[\"input\"],eval_df[\"expected\"])]\n",
    "\n",
    "print(f\"Test record found: {len(eval_dataset)}\")\n",
    "print(\"Evaluation Dataset loaded successfully!\")\n",
    "\n",
    "eval_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T16:45:53.325938300Z",
     "start_time": "2023-12-05T16:45:53.291921100Z"
    }
   },
   "id": "b02b20b98679cdd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚¨ÜÔ∏è Upload Dataset on Braintrust\n",
    "Braintrust offers the possibility to upload your Dataset to share with a team the same evaluation records. Is possible to leverage on your cloud infrastructure to store it! \n",
    "\n",
    "‚ö†Ô∏è **The possibility to upload Datasets is available only for enterprise users, so skip this cell if you do not have an enterprise subscription plan with Braintrust!**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35ae7db2671f1935"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Upload these to a new unique dataset in BrainTrust so your teammates can also easily use and manage this dataset.\n",
    "# \n",
    "\n",
    "# dataset = init_dataset(PROJECT_NAME, \n",
    "#                                   name=f\"Sentiment Evaluation for Product reviews\",\n",
    "#                                   description=\"A set of customers review samples to use for evaluating for a LLM 0-Shot task response\",\n",
    "#                                   version=\"0.1\")\n",
    "# \n",
    "# for test_case in eval_set:\n",
    "#         dataset.insert(**test_case)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a476f19c0f0f1c51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ¬©Ô∏è Prompt Setup\n",
    "In LLM-based applications, the prompt is the core part needed to address generative models' behaviour. In this notebook there different system prompts that need to be tested.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adf40991fa06933"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# üñãÔ∏è Set your system prompt here\n",
    "\n",
    "# system_prompt = \"\"\"You are the review classifier, your job is to classify customer reviews in two categories: \"POSITIVE\" or \"NEGATIVE\". Do not answer to any question and if some question arrives that is not related to a customer review respond with \"NA\" \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are the review classifier, your job is to classify customer reviews in two categories: \"POSITIVE\" or \"NEGATIVE\". If a question is posed in the input respond with \"NA\". \n",
    "Format: reply only with \"POSITIVE\", \"NEGATIVE\" or \"NA\".\n",
    "\"\"\"\n",
    "\n",
    "review_prompt = \"\"\"\n",
    "Review: {review}\n",
    "Category:\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20507efba9259f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The classifier supports only three labels as defined here\n",
    "\n",
    "SUPPORTED_LABELS = [\"POSITIVE\", \"NEGATIVE\", \"NA\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3b949a0a35069d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üß∞ Task definition \n",
    "In order to evaluate an LLM-based pipeline it is necessary define a function containing all the steps that need to be evaluated. This function will then be passed as parameter to the Eval class for to start the tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53423fa69b84d141"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generation_pipeline(input: str):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": review_prompt.format(review=input)}],\n",
    "    model=\"mistral-7b-instruct\", \n",
    "    max_tokens=50)\n",
    "  \n",
    "  return response.choices[0].message.content\n",
    "  # import numpy as np\n",
    "  # return SUPPORTED_LABELS[np.random.randint(0,3)] "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8893824ea5dbbee1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell is just to verify if all is set up correctly!\n",
    "review_input = input('Insert here your review')\n",
    "result = generation_pipeline(\"This product is fantastic?\")\n",
    "\n",
    "print(\"Result: \", result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecf6c759b980f750"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîé Metrics\n",
    "Using Braintrust is possible to access to different default metrics, more about this metrics and which ones are available is listed in the [Supported Models](https://www.braintrustdata.com/docs/guides/proxy#supported-models) guide section.\n",
    "\n",
    "Here the metrics used for this demo:\n",
    "\n",
    "| Metrics                | Type                       |\n",
    "|------------------------|----------------------------|\n",
    "| Factuality             | Model-Based Classification |\n",
    "| Levenshtein distance   | Heuristic                  |\n",
    "| Single word response   | Custom                     |\n",
    "| Supported label output | Custom                  |\n",
    "| Response format        | Custom                  |\n",
    "\n",
    "In detail:\n",
    "\n",
    "* `Factuality`: is a model-based evaluation that, using a reasoning prompt, evaluates with 0 or 1 if the actual output is coherent with the expected one;\n",
    "* `Levenshtein distance`: uses Levenshtein distance to measure the number of differences between the actual output response and the expected ones, in terms of string characters;\n",
    "* `Single Word Response`: determines the number of outputs having a single word as response (the label);\n",
    "* `Supported label output`: determines the number of outputs that aligned with the supported labels;\n",
    "* `Response format`: determines the number of outputs that follow the upper-case format.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266986b004decb5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ü§ñ Factuality is one of the model-based metrics offered by the library \n",
    "fact_evaluator = Factuality(model=\"mistral-7b-instruct\", #ü§ñ Model to use for evaluation\n",
    "                            api_key=client_key, #üóùÔ∏è LLM-provider API key\n",
    "                            base_url=\"https://braintrustproxy.com/v1\") #üõú Braintrust AI Proxy URL \n",
    "\n",
    "leven_evaluator = LevenshteinScorer()\n",
    "\n",
    "def single_response_scorer(input, output=None, expected=None):\n",
    "  if len(output.split())>1:\n",
    "    return 0\n",
    "  \n",
    "  return 1\n",
    "  \n",
    "def label_scorer(input, output=None, expected=None):\n",
    "  if output not in SUPPORTED_LABELS:\n",
    "    return 0\n",
    "  if expected not in SUPPORTED_LABELS:\n",
    "    return 0\n",
    "  \n",
    "  return 1\n",
    "\n",
    "def format_scorer(input, output=None, expected=None):\n",
    "  if not str.isupper(output):\n",
    "    return 0\n",
    "  if not str.isupper(expected):\n",
    "    return 0\n",
    "  \n",
    "  return 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8807815964346ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üò∂‚Äçüå´Ô∏è Evaluation\n",
    "As mentioned along the notebook, all the previous elements need to be passed to Braintrust Eval class. Instantiating this class allows to run the evaluation process and store all the results on Braintrust system."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5272246651814154"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "await Eval(PROJECT_NAME,\n",
    "           data=eval_set,\n",
    "           task=generation_pipeline,\n",
    "           scores=[fact_evaluator ,leven_evaluator, single_response_scorer, label_scorer, format_scorer],\n",
    "           metadata={\"experiment_name\": f\"Evaluation-{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6304e4eabce1a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
