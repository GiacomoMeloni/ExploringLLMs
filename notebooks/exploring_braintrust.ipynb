{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ðŸš€ **Exploring Braintrust Evaluation APIs**\n",
    "\n",
    "This notebook aims to explore Evaluation APIs offered by [Braintrust](https://docs.perplexity.ai/). \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GiacomoMeloni/ExploringLLMs/blob/main/notebooks/exploring_braintrust.ipynb)\n",
    "\n",
    "## ðŸ¤”  **About Braintrust**\n",
    "Braintrust is a service that helps you evaluate your LLM app, mainly focused on production-quality applications. It is possible to leverage on the [Python SDK](https://www.braintrustdata.com/docs/libs/python) as well as on the [Node library](https://www.braintrustdata.com/docs/libs/nodejs).\n",
    "\n",
    "Evaluations are crucial in AI, and accuracy deserves as much focus as speed in developing LLM-based applications. Braintrust's evaluation-driven approach enhances product reliability and quality, accelerating the testing process. This service allows teams to share evaluation datasets, with also versioning features; define their own evaluation metrics or leveraging in predefined metrics offered by the framework.\n",
    "\n",
    "## ðŸ”§ **Get Started**\n",
    "To access to `Braintrust` Dashboard an API key is needed. You can easily access through the \"Settings\" page of active account (is needed to sign up on the platform) and click on \"API Keys\" to generate a new key for your projects.\n",
    "\n",
    "âš ï¸ **Remember API Keys will be visibile only one time, after that they won't be displayed again! So keep it safe once created!**\n",
    "â„¹ï¸ **The API key needs to be stored as environment variable with the following name `BRAINTRUST_API_KEY`**\n",
    "\n",
    "Another feature that needs mention is that you need to update a GenAI Provider API Key (e.g. OpenAI, Anthropic, etc.) to leverage on some model-based metrics that we will cover later on this notebook.\n",
    "\n",
    "â„¹ï¸ **There is no need to have an enterprise plan to leverage on basic evaluation features offered by the library**\n",
    "â„¹ï¸ **For more details on API key sey-up refer to [Create an API key](https://www.braintrustdata.com/docs#create-an-api-key) guide section**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28a87194f7184aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install braintrust autoevals openai"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b6129062683654"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Basic check to get the environment in which the notebook is running\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    COLAB_ENV = True\n",
    "    from google.colab import userdata\n",
    "else:\n",
    "    COLAB_ENV = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac34d674e434181"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§‘ðŸ¼â€ðŸš€ Set up\n",
    "To access to all the functionalities offered by Braintrust we need to use a Braintrust API Key. Additionally, in this notebook is illustrated a basic test use case that leverages on OpenAI gpt-3.5-turbo model so an OpenAI API key is needed as well.\n",
    "\n",
    "ðŸ—ï¸To make this notebook work is needed to set up the following environment variables:\n",
    "\n",
    "\n",
    "| Env Variable              | Description                                          |\n",
    "|---------------------------|------------------------------------------------------|\n",
    "| `BRAINTRUST_API_KEY`      | Braintrust API key                                   |\n",
    "| `OPENAI_API_KEY`          | OpenAI API key                                       |\n",
    "| `PPLXAI_API_KEY`          | PerplexityAI API key                                 |\n",
    "| `TRUSTBRAIN_PROJECT_NAME` | The name assigned to Braintrust's Evaluation Project | "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "487f13872e893c90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from braintrust import Eval, init_dataset\n",
    "from datetime import datetime\n",
    "from autoevals import Factuality, LevenshteinScorer \n",
    "\n",
    "check_bt_api_key = bool(os.environ.get('BRAINTRUST_API_KEY')) if not COLAB_ENV else userdata.get('BRAINTRUST_API_KEY')\n",
    "\n",
    "PROJECT_NAME: str = os.environ.get('TRUSTBRAIN_PROJECT_NAME')\n",
    "\n",
    "assert check_bt_api_key, \"âŒ A Braintrust API Key it's needed for the purpose of this notebook!\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba1f9f17fab07705"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ›œ AI Proxy\n",
    "Braintrust provides also an AI proxy to access to different LLM-providers and models using a single API. For the sake of this guide is used a PerplexityAI API key. It is possible to use OpenAI, Anthropic or other providers API keys as well. \n",
    "\n",
    "â„¹ï¸ More information are provided in the official [AI Proxy](https://www.braintrustdata.com/docs/guides/proxy) guide section."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14f87f1c71efc52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For this project is used a PerplexityAI API key. It is possible to use OpenAI, Anthropic or other providers API keys as well.\n",
    "client_key = os.environ.get('PPLXAI_API_KEY', None) if not COLAB_ENV else userdata.get('PPLXAI_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://braintrustproxy.com/v1\",\n",
    "  api_key=client_key\n",
    ")\n",
    "\n",
    "assert client.api_key is not None, \"âŒ An LLM-provider API Key is needed for the purpose of this notebook!\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eddd92adeed9771"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ’¡ Use Case: Sentiment Classifier\n",
    "Imagine a scenario where a custom classifier to detect positive or negative reviews of your products. A common classification scenario that can leverage on the benefits of an evalutaion pipelines. \n",
    "\n",
    "As first step, an evaluation dataset needs to be defined. This dataset consist in list of objects composed by the following elements: \n",
    "\n",
    "* `input`: the input to evaluate, in this scenario this will be the review but for example if you need to evaluate a question answering model this might be a question;\n",
    "* `output`: the output value is supposed to be the expected application response (you can rename this as *expected* directly from your evaluation dataset to align directly to what the framework is expecting to receive);\n",
    "* `metadata`: is a set of key-value pairs that you can use to filter and group your data.\n",
    "\n",
    "These data will be used to evaluate the LLM-based app later! \n",
    "\n",
    "â„¹ï¸ For further information about Evaluation Datasets look at [Datasets](https://www.braintrustdata.com/docs/guides/datasets) page in the official guide."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea473de609260676"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_set = [\n",
    "  {\n",
    "    \"input\": \"The product exceeded my expectations. Even Gandalf would love it!\",\n",
    "    \"expected\": \"POSITIVE\"\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"How's the weather?\",\n",
    "    \"expected\": \"NA\"\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Terrible experience with this product. I would not recommend it.\",\n",
    "    \"expected\": \"NEGATIVE\"\n",
    "  }, \n",
    "  {\n",
    "    \"input\": \"The service was not professional, how could you offer this experience to your clients?\",\n",
    "    \"expected\": \"NEGATIVE\"\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Great customer service, very helpful and responsive.\",\n",
    "    \"expected\": \"POSITIVE\"\n",
    "  }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02b20b98679cdd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## â¬†ï¸ Upload Dataset\n",
    "Braintrust offers the possibility to upload your Dataset to share with a team the same evaluation records. Is possible to leverage on your cloud infrastructure to store it! \n",
    "\n",
    "âš ï¸ **The possibility to upload Datasets is available only for enterprise users, so skip this cell if you do not have an enterprise subscription plan with Braintrust!**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35ae7db2671f1935"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Upload these to a new unique dataset in BrainTrust so your teammates can also easily use and manage this dataset.\n",
    "# \n",
    "\n",
    "# dataset = init_dataset(PROJECT_NAME, \n",
    "#                                   name=f\"Sentiment Evaluation for Product reviews\",\n",
    "#                                   description=\"A set of customers review samples to use for evaluating for a LLM 0-Shot task response\",\n",
    "#                                   version=\"0.1\")\n",
    "# \n",
    "# for test_case in eval_set:\n",
    "#         dataset.insert(**test_case)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a476f19c0f0f1c51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Â©ï¸ Prompt Setup\n",
    "In LLM-based applications, the prompt is the core part needed to address generative models' behaviour. In this notebook there different system prompts that need to be tested.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adf40991fa06933"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ðŸ–‹ï¸ Set your system prompt here\n",
    "\n",
    "# system_prompt = \"\"\"You are the review classifier, your job is to classify customer reviews in two categories: \"POSITIVE\" or \"NEGATIVE\". Do not answer to any question and if some question arrives that is not related to a customer review respond with \"NA\" \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are the review classifier, your job is to classify customer reviews in two categories: \"POSITIVE\" or \"NEGATIVE\". If a question is posed in the input respond with \"NA\". \n",
    "Format: reply only with \"POSITIVE\", \"NEGATIVE\" or \"NA\".\n",
    "\"\"\"\n",
    "\n",
    "review_prompt = \"\"\"\n",
    "Review: {review}\n",
    "Category:\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20507efba9259f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The classifier supports only three labels as defined here\n",
    "\n",
    "SUPPORTED_LABELS = [\"POSITIVE\", \"NEGATIVE\", \"NA\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3b949a0a35069d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§° Task definition \n",
    "In order to evaluate an LLM-based pipeline it is necessary define a function containing all the steps that need to be evaluated. This function will then be passed as parameter to the Eval class for to start the tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53423fa69b84d141"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generation_pipeline(input: str):\n",
    "  response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": review_prompt.format(review=input)}],\n",
    "    model=\"mistral-7b-instruct\", \n",
    "    max_tokens=50)\n",
    "  \n",
    "  return response.choices[0].message.content\n",
    "  # import numpy as np\n",
    "  # return SUPPORTED_LABELS[np.random.randint(0,3)] "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8893824ea5dbbee1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell is just to verify if all is set up correctly!\n",
    "review_input = input('Insert here your review')\n",
    "result = generation_pipeline(\"This product is fantastic?\")\n",
    "\n",
    "print(\"Result: \", result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecf6c759b980f750"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Metrics                   | Type                      | Status                |\n",
    "|---------------------------|---------------------------|-----------------------|\n",
    "| Factuality                | Model-Based Classification | Available             |\n",
    "| Levenshtein distance      | Heuristic                  | Available             |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266986b004decb5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ðŸ¤– Factuality is one of the model-based metrics offered by the library \n",
    "fact_evaluator = Factuality(model=\"mistral-7b-instruct\",\n",
    "                            api_key=client_key,\n",
    "                            base_url=\"https://braintrustproxy.com/v1\") \n",
    "leven_evaluator = LevenshteinScorer()\n",
    "\n",
    "def single_response_scorer(input, output=None, expected=None):\n",
    "  if len(output.split())>1:\n",
    "    return 0\n",
    "  \n",
    "  return 1\n",
    "  \n",
    "def label_scorer(input, output=None, expected=None):\n",
    "  if output not in SUPPORTED_LABELS:\n",
    "    return 0\n",
    "  if expected not in SUPPORTED_LABELS:\n",
    "    return 0\n",
    "  \n",
    "  return 1\n",
    "\n",
    "def format_scorer(input, output=None, expected=None):\n",
    "  if not str.isupper(output):\n",
    "    return 0\n",
    "  if not str.isupper(expected):\n",
    "    return 0\n",
    "  \n",
    "  return 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8807815964346ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "await Eval(PROJECT_NAME,\n",
    "           data=eval_set,\n",
    "           task=generation_pipeline,\n",
    "           scores=[fact_evaluator ,leven_evaluator, single_response_scorer, label_scorer, format_scorer],\n",
    "           metadata={\"experiment_name\": f\"Evaluation-{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6304e4eabce1a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5fa539189f38d141"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
